"""
Nadoo Flow - Advanced Features Example
ìƒˆë¡œ ì¶”ê°€ëœ ê¸°ëŠ¥ë“¤(Retry, Fallback, Parser, Caching, Callbacks) ì‚¬ìš© ì˜ˆì œ
"""

import asyncio
from typing import Any, Literal
from pydantic import BaseModel, Field

from nadoo_flow import (
    # Core
    BaseNode,
    NodeContext,
    NodeResult,
    WorkflowContext,
    WorkflowExecutor,
    # Resilience
    RetryableNode,
    RetryPolicy,
    FallbackNode,
    # Parsers
    StructuredOutputParser,
    ParserNode,
    RetryableParserNode,
    # Callbacks
    CallbackManager,
    ConsoleCallbackHandler,
    LoggingCallbackHandler,
    # Caching
    InMemoryCache,
    ResponseCache,
    CachedNode,
)


# ============================================================================
# 1. Retry ë©”ì»¤ë‹ˆì¦˜ ì˜ˆì œ
# ============================================================================

class UnreliableLLMNode(RetryableNode):
    """ì¬ì‹œë„ ê¸°ëŠ¥ì´ ìˆëŠ” LLM ë…¸ë“œ ì˜ˆì œ

    ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ë‚˜ ì¼ì‹œì ì¸ ì¥ì•  ì‹œ ìë™ìœ¼ë¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤.
    """

    def __init__(self):
        super().__init__(
            node_id="unreliable_llm",
            node_type="llm",
            name="Unreliable LLM with Retry",
            config={},
            retry_policy=RetryPolicy(
                max_attempts=5,
                initial_delay=1.0,
                max_delay=30.0,
                exponential_base=2.0,
                jitter=1.0,
                retry_on_exceptions=(TimeoutError, ConnectionError)
            )
        )
        self.call_count = 0

    async def _execute_with_retry(
        self,
        node_context: NodeContext,
        workflow_context: WorkflowContext
    ) -> NodeResult:
        """ì‹¤ì œ LLM í˜¸ì¶œ ë¡œì§ (ì¬ì‹œë„ ì—†ì´)"""
        self.call_count += 1

        # ì‹œë®¬ë ˆì´ì…˜: ì²˜ìŒ 2ë²ˆì€ ì‹¤íŒ¨
        if self.call_count <= 2:
            print(f"   Attempt {self.call_count} failed (simulated)")
            raise TimeoutError("Simulated timeout")

        # 3ë²ˆì§¸ ì‹œë„ì—ì„œ ì„±ê³µ
        print(f"  âœ… Attempt {self.call_count} succeeded!")
        return NodeResult(
            success=True,
            output={"text": "LLM response after retries"}
        )


async def demo_retry():
    """Retry ë©”ì»¤ë‹ˆì¦˜ ë°ëª¨"""
    print("\n" + "=" * 60)
    print("1. RETRY MECHANISM DEMO")
    print("=" * 60)

    node = UnreliableLLMNode()
    context = WorkflowContext()
    node_context = NodeContext(
        node_id=node.node_id,
        node_type=node.node_type
    )

    result = await node.execute(node_context, context)

    print(f"\nâœ… Final Result: {result.success}")
    print(f"   Total attempts: {result.metadata.get('retry_info', {}).get('total_attempts')}")


# ============================================================================
# 2. Fallback ë…¸ë“œ ì˜ˆì œ
# ============================================================================

class GPT4Node(BaseNode):
    """GPT-4 ì‹œë®¬ë ˆì´ì…˜ (ë¹„ì‹¸ì§€ë§Œ ì¢‹ìŒ)"""

    def __init__(self):
        super().__init__(
            node_id="gpt4",
            node_type="llm",
            name="GPT-4",
            config={}
        )

    async def execute(self, node_context: NodeContext, workflow_context: WorkflowContext) -> NodeResult:
        # ì‹œë®¬ë ˆì´ì…˜: Rate limit ì—ëŸ¬
        print("  ğŸ”´ GPT-4: Rate limit exceeded!")
        raise Exception("Rate limit exceeded")


class ClaudeNode(BaseNode):
    """Claude ì‹œë®¬ë ˆì´ì…˜ (ì¤‘ê°„)"""

    def __init__(self):
        super().__init__(
            node_id="claude",
            node_type="llm",
            name="Claude",
            config={}
        )

    async def execute(self, node_context: NodeContext, workflow_context: WorkflowContext) -> NodeResult:
        # ì‹œë®¬ë ˆì´ì…˜: Timeout
        print("  ğŸŸ¡ Claude: Timeout!")
        raise TimeoutError("Request timeout")


class LocalLlamaNode(BaseNode):
    """Local Llama ì‹œë®¬ë ˆì´ì…˜ (ì €ë ´í•˜ê³  ì•ˆì •ì )"""

    def __init__(self):
        super().__init__(
            node_id="llama",
            node_type="llm",
            name="Local Llama",
            config={}
        )

    async def execute(self, node_context: NodeContext, workflow_context: WorkflowContext) -> NodeResult:
        # í•­ìƒ ì„±ê³µ
        print("  ğŸŸ¢ Local Llama: Success!")
        return NodeResult(
            success=True,
            output={"text": "Response from local Llama model"}
        )


async def demo_fallback():
    """Fallback ë…¸ë“œ ë°ëª¨"""
    print("\n" + "=" * 60)
    print("2. FALLBACK NODE DEMO")
    print("=" * 60)

    # Fallback ì²´ì¸: GPT-4 â†’ Claude â†’ Local Llama
    fallback = FallbackNode(
        node_id="llm_fallback",
        nodes=[
            GPT4Node(),
            ClaudeNode(),
            LocalLlamaNode()
        ],
        handle_exceptions=(Exception,)
    )

    context = WorkflowContext()
    node_context = NodeContext(
        node_id=fallback.node_id,
        node_type=fallback.node_type
    )

    result = await fallback.execute(node_context, context)

    print(f"\nâœ… Final Result: {result.success}")
    print(f"   Successful node: {result.metadata.get('fallback_info', {}).get('successful_node')}")
    print(f"   Fallback index: {result.metadata.get('fallback_info', {}).get('fallback_index')}")


# ============================================================================
# 3. Structured Output Parser ì˜ˆì œ
# ============================================================================

class AgentAction(BaseModel):
    """ì—ì´ì „íŠ¸ í–‰ë™ ëª¨ë¸"""

    action: Literal["search", "calculate", "answer"]
    reasoning: str = Field(description="Why this action was chosen")
    parameters: dict[str, Any] = Field(default_factory=dict)
    confidence: float = Field(ge=0.0, le=1.0, default=0.5)

    class Config:
        json_schema_extra = {
            "example": {
                "action": "search",
                "reasoning": "User wants to know about AI",
                "parameters": {"query": "What is AI?"},
                "confidence": 0.9
            }
        }


class MockLLMNode(BaseNode):
    """LLM ì¶œë ¥ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” ë…¸ë“œ"""

    def __init__(self, response: str):
        super().__init__(
            node_id="mock_llm",
            node_type="llm",
            name="Mock LLM",
            config={}
        )
        self.response = response

    async def execute(self, node_context: NodeContext, workflow_context: WorkflowContext) -> NodeResult:
        return NodeResult(
            success=True,
            output={"text": self.response}
        )


async def demo_parser():
    """Structured Output Parser ë°ëª¨"""
    print("\n" + "=" * 60)
    print("3. STRUCTURED OUTPUT PARSER DEMO")
    print("=" * 60)

    # LLM ì‘ë‹µ ì‹œë®¬ë ˆì´ì…˜ (JSON í˜•ì‹)
    llm_output = """
    Here's my decision:
    ```json
    {
        "action": "search",
        "reasoning": "The user wants to find information about quantum computing",
        "parameters": {
            "query": "quantum computing basics",
            "max_results": 5
        },
        "confidence": 0.85
    }
    ```
    """

    # íŒŒì„œ ìƒì„±
    parser = StructuredOutputParser(pydantic_model=AgentAction)

    # íŒŒì„œ ë…¸ë“œ ìƒì„±
    parser_node = ParserNode(
        node_id="action_parser",
        parser=parser,
        input_key="text"
    )

    # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
    llm_node = MockLLMNode(llm_output)
    executor = WorkflowExecutor()
    executor.add_node(llm_node)

    context = WorkflowContext()
    node_context = NodeContext(
        node_id=llm_node.node_id,
        node_type=llm_node.node_type
    )

    # LLM ì‹¤í–‰
    llm_result = await llm_node.execute(node_context, context)
    print(f"\nğŸ“ LLM Output:\n{llm_result.output['text'][:200]}...")

    # íŒŒì‹±
    parser_context = NodeContext(
        node_id=parser_node.node_id,
        node_type=parser_node.node_type,
        input_data=llm_result.output
    )
    parse_result = await parser_node.execute(parser_context, context)

    if parse_result.success:
        parsed = parse_result.output["parsed"]
        print(f"\nâœ… Parsed Action:")
        print(f"   Action: {parsed['action']}")
        print(f"   Reasoning: {parsed['reasoning']}")
        print(f"   Confidence: {parsed['confidence']}")
        print(f"   Parameters: {parsed['parameters']}")


# ============================================================================
# 4. LLM Response Caching ì˜ˆì œ
# ============================================================================

class CachedLLMNode(BaseNode, CachedNode):
    """ìºì‹± ê¸°ëŠ¥ì´ ìˆëŠ” LLM ë…¸ë“œ"""

    def __init__(self, cache: ResponseCache):
        BaseNode.__init__(
            self,
            node_id="cached_llm",
            node_type="llm",
            name="Cached LLM",
            config={}
        )
        CachedNode.__init__(self, response_cache=cache)
        self.call_count = 0

    async def execute(self, node_context: NodeContext, workflow_context: WorkflowContext) -> NodeResult:
        prompt = node_context.get_input("prompt", "")

        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = self.response_cache.make_key(
            prompt=prompt,
            model="gpt-4",
            temperature=0.7
        )

        # ìºì‹œ ì¡°íšŒ
        if self.is_cache_enabled():
            cached = self.response_cache.get(cache_key)
            if cached:
                print(f"  ğŸ’¾ Cache HIT for: {prompt[:50]}...")
                return NodeResult(success=True, output=cached)

        # ìºì‹œ ë¯¸ìŠ¤ - LLM í˜¸ì¶œ
        print(f"  ğŸ”„ Cache MISS - Calling LLM for: {prompt[:50]}...")
        self.call_count += 1

        # ì‹œë®¬ë ˆì´ì…˜: LLM í˜¸ì¶œ (1ì´ˆ ì†Œìš”)
        await asyncio.sleep(1)
        response = f"LLM response to: {prompt}"

        output = {"text": response, "call_count": self.call_count}

        # ìºì‹œ ì €ì¥
        if self.is_cache_enabled():
            self.response_cache.set(cache_key, output, ttl=3600)

        return NodeResult(success=True, output=output)


async def demo_caching():
    """LLM Response Caching ë°ëª¨"""
    print("\n" + "=" * 60)
    print("4. LLM RESPONSE CACHING DEMO")
    print("=" * 60)

    # ìºì‹œ ì„¤ì •
    cache = ResponseCache(
        cache=InMemoryCache(default_ttl=3600),
        namespace="demo"
    )

    node = CachedLLMNode(cache)
    context = WorkflowContext()

    # ì²« ë²ˆì§¸ í˜¸ì¶œ (ìºì‹œ ë¯¸ìŠ¤)
    print("\nğŸ“¤ First call:")
    node_context1 = NodeContext(
        node_id=node.node_id,
        node_type=node.node_type,
        input_data={"prompt": "What is artificial intelligence?"}
    )
    result1 = await node.execute(node_context1, context)
    print(f"   Response: {result1.output['text']}")

    # ë‘ ë²ˆì§¸ í˜¸ì¶œ - ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ (ìºì‹œ íˆíŠ¸)
    print("\nğŸ“¥ Second call (same prompt):")
    node_context2 = NodeContext(
        node_id=node.node_id,
        node_type=node.node_type,
        input_data={"prompt": "What is artificial intelligence?"}
    )
    result2 = await node.execute(node_context2, context)
    print(f"   Response: {result2.output['text']}")

    # ì„¸ ë²ˆì§¸ í˜¸ì¶œ - ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ (ìºì‹œ ë¯¸ìŠ¤)
    print("\nğŸ“¤ Third call (different prompt):")
    node_context3 = NodeContext(
        node_id=node.node_id,
        node_type=node.node_type,
        input_data={"prompt": "Explain machine learning"}
    )
    result3 = await node.execute(node_context3, context)
    print(f"   Response: {result3.output['text']}")

    print(f"\nğŸ“Š Total LLM API calls: {node.call_count} (saved 1 call via cache)")


# ============================================================================
# 5. Callback System ì˜ˆì œ
# ============================================================================

async def demo_callbacks():
    """Callback System ë°ëª¨"""
    print("\n" + "=" * 60)
    print("5. CALLBACK SYSTEM DEMO")
    print("=" * 60)

    # ì½œë°± ë§¤ë‹ˆì € ì„¤ì •
    callback_manager = CallbackManager()
    callback_manager.add_handler(ConsoleCallbackHandler(verbose=True, colors=True))

    # ê°„ë‹¨í•œ ì›Œí¬í”Œë¡œìš° ìƒì„±
    class SimpleNode(BaseNode):
        async def execute(self, node_context: NodeContext, workflow_context: WorkflowContext) -> NodeResult:
            await asyncio.sleep(0.5)  # ì‘ì—… ì‹œë®¬ë ˆì´ì…˜
            return NodeResult(success=True, output={"result": "completed"})

    node = SimpleNode(
        node_id="simple_node",
        node_type="custom",
        name="Simple Node",
        config={}
    )

    context = WorkflowContext()
    node_context = NodeContext(
        node_id=node.node_id,
        node_type=node.node_type
    )

    # ì›Œí¬í”Œë¡œìš° ì‹œì‘ ì´ë²¤íŠ¸
    callback_manager.on_workflow_start(context, inputs={"test": "data"})

    # ë…¸ë“œ ì‹œì‘ ì´ë²¤íŠ¸
    callback_manager.on_node_start(node_context, context)

    # ë…¸ë“œ ì‹¤í–‰
    result = await node.execute(node_context, context)

    # ë…¸ë“œ ì¢…ë£Œ ì´ë²¤íŠ¸
    from nadoo_flow import NodeStatus
    node_context.end_time = node_context.start_time + 0.5
    node_context.status = NodeStatus.SUCCESS
    callback_manager.on_node_end(node_context, context, result)

    # ì›Œí¬í”Œë¡œìš° ì¢…ë£Œ ì´ë²¤íŠ¸
    context.status = NodeStatus.SUCCESS
    context.end_time = context.start_time + 1.0
    callback_manager.on_workflow_end(context)


# ============================================================================
# Main
# ============================================================================

async def main():
    """ëª¨ë“  ë°ëª¨ ì‹¤í–‰"""
    print("\n" + "=" * 60)
    print("NADOO FLOW - ADVANCED FEATURES DEMO")
    print("=" * 60)

    await demo_retry()
    await demo_fallback()
    await demo_parser()
    await demo_caching()
    await demo_callbacks()

    print("\n" + "=" * 60)
    print("âœ… ALL DEMOS COMPLETED")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
